# Unintentional Bias: Discussion

### Discussion Questions

- How have you seen crime data used in the media?
- Where do you think crime data comes from?
- How might crime data be biased?

## Crimes vs. Arrests vs. Crimes

| From an editorial in the [Chicago Tribune (May 25, 2018)](https://www.chicagotribune.com/opinion/commentary/ct-perspec-danger-marijuana-legalizing-crime-data-black-youth-facial-bias-0528-story.html) |
| :--- |
| <p>Only two thirds of murders result in arrests, which means that the homicide data are missing at least a third of actual incidents. And murders are unusual in that we typically have the body, so we know a crime actually occurred. That's not the case with assaults, rapes, thefts or illegal gun possession. There's no reason to think that the majority of these crimes lead to arrests, or that all arrests are related to actual crimes.</p><p>Reports aren't much better. People decide whether to report in a cultural context. For example, they're more likely to do so if they trust the police, and the level of trust can vary sharply over time. A year after Donald Trump was elected president, the number of reported rapes among the Latino population of Houston declined by 40 percent, a strong indication that people became afraid to report the crimes. Police often don't take rape victims' reports seriously, a problem that is probably even worse for male victims.</p> |

### Discussion Questions

- What forms of bias are discussed in the passage above?
- What do you think the author believes about crime data?
- How does the author consider missing or absent data in her analysis?
- JOLSON QUESTIONS

## The Presidential Election of 1936

The popular magazine _Literary Digest_ commissioned a pre-election survey in 1936 in order to predict the winner of the upcoming Presidential election between Alfred Landon, Republican Governor of Kansas, and Franklin D. Roosevelt, the incumbent Democratic President. Although you probably know who ultimately won the election, we're going to examine the survey results collected by the _Literary Digest_.

Make a copy of this Google Sheet, a representative subset of the full dataset: [https://docs.google.com/spreadsheets/d/1e7aqBiZEgnJV0pjK0yA9J4DPgRNAuwVeVuTmBJJqGgI/edit?usp=sharing](https://docs.google.com/spreadsheets/d/1e7aqBiZEgnJV0pjK0yA9J4DPgRNAuwVeVuTmBJJqGgI/edit?usp=sharing)

Notes about the data collection:
- Based on every telephone directory in the United States, lists of magazine subscribers, rosters of clubs and associations, and other sources, a mailing list of about 10 million names was created.
- The list of 10 million names represented every county in the United States.
- Every name on this list was mailed a mock ballot and asked to return the marked ballot to the magazine.
- Around 2.4 million ballots were returned to the _Literary Digest_.

### Calculate
- Based on the data, who did the _Literary Digest_ predict would win the 1936 election?
- By what margin did the _Literary Digest_ predict the winner would win?

### Discussion Questions

The actual result of the election was 62% for Roosevelt vs. 38% for Landon, a 24-point spread.

- How could the _Literary Digest_ have gotten their poll so wrong?
- What sources of bias can you see in the data collection?
	> Hint: How were participants selected? and what was the response rate?
- How would those sources of bias have impacted the data collected?
- What could the _Literary Digest_ have done to mitigate the impacts of those biases?
- JOLSON QUESTIONS

#### Additional Reading

- Based on [Case Study 1: 1936 _Literary Digest_ Poll](https://www.math.upenn.edu/~deturck/m170/wk4/lecture/case1.html) 

## Summary Questions

- Today you've seen two ways that unintentional bias can skew the insight we get from data. Make a list of ways that you will avoid unintentional bias in your data project(s).
- JOLSON QUESTIONS
